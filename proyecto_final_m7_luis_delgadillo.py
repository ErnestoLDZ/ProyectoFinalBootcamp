# -*- coding: utf-8 -*-
"""Proyecto final M7 LUIS DELGADILLO

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mTjeoWovt1kPyRzsEFPXWGMJrNSaeICp

## **Bootcamp: Ciencia de Datos e Inteligencia Artificial**
## **Proyecto del Módulo 7: Técnicas avanzadas para ciencia de datos y empleabilidad**


presentado por:
#          **Luis Ernesto Delgadillo Zaldivar.**

---

En este trabajo haremos un EDA del dataset GOOGLE PLAY STORE y analizaremos datos de interés para poder generar una IA que nos ayude a predecir los gustos de los usuarios basandonos en sus comentarios y rankeo.

##Preprocesamiento de datos.
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf
import matplotlib.pyplot as plt
from collections import defaultdict

# Visualización
import seaborn as sns
import matplotlib.pyplot as plt
import missingno as msn
from wordcloud import WordCloud


# Preprocesamiento
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler

# Regresión
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor

# Clasificación
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

# Metricas
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import accuracy_score, confusion_matrix


import warnings
warnings.filterwarnings('ignore')

#Descargamos Dataset y comenzamos a limpiar los datos.
DF_GP= pd.read_csv('/content/googleplaystore.csv', delimiter=',', encoding = "latin" )

DF_GP.info()

DF_GP.head()

DF_GP = DF_GP.drop_duplicates()

DF_GP.isnull().sum()



DF_GP['Rating'].unique()

sns.boxplot(DF_GP['Rating'])

np.mean(DF_GP['Rating'])

DF_GP['Rating'] = DF_GP['Rating'].fillna(np.mean(DF_GP['Rating']))

DF_GP['Rating'].isnull().sum()

#En la columna REVIEWS buscamos los valores que no son únicamente númericos.

DF_GP[~DF_GP['Reviews'].str.isnumeric()]

#Modificamos nuestro valor y continuamos con la siguiente columna.

DF_GP['Reviews'] = DF_GP['Reviews'].replace('3.0M','3000000')

DF_GP['Size'].unique()

#En la columna SIZE notamos que tenemos k y M refiriendose a Kilobyte y Megabyte respectivamente.
#Convertiremos toda la data en kb.
# 1 mb = 1024 kb

def convert_to_kb(size):
    if 'M' in size:

        mb_value = size.split('M')[0]
        kb_value = float(mb_value) * 1024  # 1 MB = 1024 KB
        kb_value = str(f'{kb_value} k')
        return kb_value
    else:
        return size

DF_GP['Size'] = DF_GP['Size'].apply(convert_to_kb)

DF_GP['Size'].unique()

#Eliminamos la k para unicamente visualizar las cantidades.
DF_GP['Size']=DF_GP['Size'].str.replace('k','')

DF_GP['Size'].unique()

#Eliminamos valor no numérico.
DF_GP = DF_GP[~DF_GP['Size'].str.contains('Varies with device')]

DF_GP = DF_GP.reset_index(drop=True)

DF_GP['Size'].unique()

DF_GP['Size']=DF_GP['Size'].replace('1,000+','1000')
DF_GP['Size']=DF_GP['Size'].astype(float)

DF_GP['Size'].isnull().sum()

sns.boxplot(DF_GP['Size'])

median_size = DF_GP['Size'].median()
DF_GP['Size'] = DF_GP['Size'].fillna(median_size)

DF_GP['Size'].isnull().sum()

sns.boxplot(DF_GP['Size'])

DF_GP['Price'].unique()

DF_GP['Installs'].unique()

#Limpiamos símbolos, valores no numéricos y valores faltantes.
chars_to_remove = ['+',',','$']
coloumns_to_clean = ['Installs','Price']
for item in chars_to_remove:
    for coloumns in coloumns_to_clean:
        DF_GP[coloumns] = DF_GP[coloumns].str.replace(item,'')

DF_GP['Price'] = DF_GP['Price'].replace('Everyone',0)

DF_GP['Price'] = DF_GP['Price'].astype(float)

DF_GP['Installs'] = DF_GP['Installs'].replace('Free',np.nan)

DF_GP['Installs'].isnull().sum()

DF_GP['Installs'].unique()

installs_median = DF_GP['Installs'].median()

DF_GP['Installs'] = DF_GP['Installs'].fillna(installs_median)

DF_GP['Installs']=DF_GP['Installs'].astype(int)

DF_GP['Installs'].isnull().sum()

#Modificamos y limpiamos columna Last Updated.
DF_GP['last_Updated_Month']=DF_GP['Last Updated'].str.split(' ').str[0]
DF_GP['last_Updated_Day']=DF_GP['Last Updated'].str.split(' ').str[1]
DF_GP['last_Updated_Day']=DF_GP['last_Updated_Day'].str.split(',').str[0]
DF_GP['last_Updated_year']=DF_GP['Last Updated'].str.split(',').str[1]

DF_GP['last_Updated_Month'].unique()

DF_GP['last_Updated_Month'] = DF_GP['last_Updated_Month'].replace('1.0.19','October')

DF_GP['last_Updated_Day'].unique()

DF_GP['last_Updated_Day'] = DF_GP['last_Updated_Day'].fillna(1)

DF_GP['last_Updated_year'].unique()

DF_GP['last_Updated_year'] = DF_GP['last_Updated_year'].fillna(2019)

DF_GP = DF_GP.drop('Last Updated',axis = 1)

DF_GP.dtypes

DF_GP['Reviews'] = DF_GP['Reviews'].astype(float)
DF_GP['last_Updated_Day'] = DF_GP['last_Updated_Day'].astype(float)
DF_GP['last_Updated_year'] = DF_GP['last_Updated_year'].astype(float)

#Ordenamos los meses del año.
DF_GP['last_Updated_Month']=DF_GP['last_Updated_Month'].map({'January':1,'February':2,'March': 3,'April': 4,'May': 5,'June': 6,'July': 7,'August': 8,'September': 9,'October': 10,'November': 11,'December': 12})

DF_GP.dtypes

DF_GP.isnull().sum()

DF_GP['Type'] = DF_GP['Type'].fillna(DF_GP['Type'].mode().iloc[0])

DF_GP.isnull().sum()

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

DF_GP['Type'].unique()

DF_GP['Type'] = DF_GP['Type'].replace('0','Free')

DF_GP['Type'] = encoder.fit_transform(DF_GP['Type'])

DF_GP['Content Rating'].unique()

DF_GP['Content Rating'] = encoder.fit_transform(DF_GP['Content Rating'])

DF_GP['Content Rating'].unique()

#Iniciamos limpieza del segundo Dataset y unimos con el primero.
DF_GP_reviews= pd.read_csv('/content/googleplaystore_user_reviews.csv', delimiter=',', encoding = "latin" )

DF_GP_reviews.info()

DF_GP_reviews.head()
#Polaridad del sentimiento: 0 = Neutral, > 0 = Positivo, < 0 = Negativo.

DF_GP_reviews = DF_GP_reviews.drop_duplicates()

#Unión de datasets en uno sólo.
merged_df = pd.merge(DF_GP, DF_GP_reviews, how='inner')

display(merged_df)

DF_sentimiento=merged_df[['Translated_Review','Sentiment',"App"]].dropna().copy()

DF_sentimiento['Sentiment'].unique()

DF_sentimiento

DF_sentimiento['Sentiment'] = encoder.fit_transform(DF_sentimiento['Sentiment']).copy()

DF_sentimiento['Sentiment'].unique()

#Prepocesamiento de nuevo dataset, con una "técnica" distinta a la anterior.
import re

def preprocesamiento (review):
  review = re.sub ( re.compile ('((http|https):\/\/[\w\-_]+(\.[\w\-_]+)+([\w\-\.,@?^=%&amp;:/~\+#]*[\w\-\@?^=%&amp;/~\+#])?)'), ' ', review)
  review = re.sub ( re.compile ('\d'), ' ', review)
  review = re.sub ( re.compile ('[,;.:¡!¿?@#$%&[\](){}<>~=+\-*/|\\_^`"\']'), ' ', review)
  review = re.sub ( re.compile (' +'), ' ', review)

  return review

DF_sentimiento['Sentiment'] = DF_sentimiento['Sentiment'].astype(float).copy()

DF_sentimiento['Translated_Review'][42]

preprocesamiento(DF_sentimiento['Translated_Review'][42])

#Ahora, importamos NLTK para utilizar el tokenizador y eliminar las stopwords.

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('stopwords')

#Definimos nuestra función para tokenizar y eliminar stopwords.

def token_stop (review):
  tokens = nltk.word_tokenize (review) #Creamos vocabulario
  stop_words = stopwords.words ('english')
  lista_tokens = [token for token in tokens if token not in stop_words]
  return' '.join(lista_tokens)

#Aplicamos las funciones de preprocesamiento y token_stop a todos las reviewa del dataset.
#Creamos Df con todos nuestros textos limpios, terminando así la limpieza de datos.

DF_sentimiento['reviewlimpio']=DF_sentimiento['Translated_Review'].apply(lambda review:token_stop(preprocesamiento(review)))

#Generamos gráfica para ver la distribución de los reviews.

DF_sentimiento['Sentiment'].value_counts().plot(kind='bar')
plt.title('Distribución de etiquetas' )
plt.xlabel('Etiquetas Positivo=2, Neutro=1, Negativo=0 ')
plt.ylabel('Cantidad de etiquetas')

#Guardar imagen en computadora
plt.savefig("my_plot2.jpg")
plt.show()

from wordcloud import WordCloud

#Las palabras más usadas en los reviews.
plt.subplots(figsize=(5,5))
wordcloud = WordCloud(background_color='black',width=1920,height=1080).generate(" ".join(DF_sentimiento.reviewlimpio))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

DF_sentimiento.head()



"""##Análisis de datos."""

# Extraemos columnas relevantes para conocer los meses donde la gente tiene mayor actividad en la google store.

launch_data = DF_GP[['last_Updated_Month', 'last_Updated_Day', 'last_Updated_year', 'Installs', 'Rating', 'Reviews']]

# Analisis de las series de tiempo
monthly_avg_installs = launch_data.groupby('last_Updated_Month')['Installs'].mean()
monthly_avg_ratings = launch_data.groupby('last_Updated_Month')['Rating'].mean()
monthly_avg_reviews = launch_data.groupby('last_Updated_Month')['Reviews'].mean()

# Plots de las series de tiempo
plt.figure(figsize=(12, 6))
plt.subplot(311)
monthly_avg_installs.plot(kind='line', marker='o', color='skyblue')
plt.title('Average Installs Over Months')
plt.xlabel('Month')
plt.ylabel('Average Installs')

plt.subplot(312)
monthly_avg_ratings.plot(kind='line', marker='o', color='orange')
plt.title('Average Ratings Over Months')
plt.xlabel('Month')
plt.ylabel('Average Rating')

plt.subplot(313)
monthly_avg_reviews.plot(kind='line', marker='o', color='purple')
plt.title('Average Reviews Over Months')
plt.xlabel('Month')
plt.ylabel('Average Reviews')

plt.tight_layout()
plt.show()

# Análisis por categoria
category_counts = DF_GP['Category'].value_counts()
# Distribución de las apps por categoria
plt.figure(figsize=(12, 6))
plt.bar(category_counts.index, category_counts.values, color='skyblue')
plt.xticks(rotation=90)
plt.xlabel('Categoria')
plt.ylabel('Número de Apps')
plt.title('Apps por categoria')
plt.tight_layout()
plt.show()

# Análisis por instalaciones
install_counts = DF_GP.groupby('Category')['Installs'].sum().sort_values(ascending=False)
# Plot de instalaciones totales
plt.figure(figsize=(12, 6))
plt.bar(install_counts.index, install_counts.values, color='green')
plt.xticks(rotation=90)
plt.xlabel('Categoria')
plt.ylabel('Instalaciones')
plt.title('Instalaciones totales por categoria')
plt.tight_layout()
plt.show()

# Identificamos la categoria más populares basandonos en su categoria
most_popular_category = install_counts.idxmax()
print(f"The most popular app category is '{most_popular_category}'")

#Número de apps con calificación 5 estrellas
df_rating_5 = DF_GP[DF_GP.Rating == 5.]
print(f'There are {df_rating_5.shape[0]} apps having rating of 5.0')

df_rating_5_cat =  df_rating_5['Category'].value_counts().reset_index()

# Top 10 categorias con más apps calificadas con 5
category = pd.DataFrame(df_rating_5['Category'].value_counts())
category.rename(columns = {'Category':'Count'},inplace=True)
category

df_rating_5_type =  df_rating_5['Type'].value_counts().reset_index()

# Creamos un pie chart
plt.figure(figsize=(7, 5))
sns.set(style="whitegrid")

# Datos para gráfica
sizes = df_rating_5_type.iloc[:, 1]
labels = df_rating_5_type.iloc[:, 0]

# Muestra resaltada de un valor.
explode = (0, 0.1)  # Ajustamos el segundo valor para ajustar  "resaltado"

plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, pctdistance=0.85, explode=explode)

# Genera la gráfica como dona.
centre_circle = plt.Circle((0,0),0.70,fc='white')
fig = plt.gcf()
fig.gca().add_artist(centre_circle)

# Aseguramos sea dibujado como un círculo perfecto.
plt.axis('equal')

plt.title('Pie chart de Apps con calificación 5.0 gratuitas vs pagadas')

plt.show()

#Apps con más descargas y Rating 5
DF_Canti = df_rating_5.sort_values(by='Installs', ascending=False)


top_10_cat = DF_Canti[['App','Rating',"Installs"]].head(10)

top_10_cat = top_10_cat.reset_index(drop=True)

print(top_10_cat)

df_largest_size = DF_GP.groupby(['App'])['Size'].sum().sort_values(ascending = False).reset_index()
df_largest_size.Size = df_largest_size.Size/1024# Convertimos a MB para mejor represntación de esta gráfica
df2 = df_largest_size.head(10)
plt.figure(figsize = (8,5))
sns.set_context("talk")
sns.set_style("darkgrid")
ax = sns.barplot(x = 'Size' , y = 'App' , data = df2 )
ax.set_xlabel('Size in MB')
ax.set_ylabel('')
ax.set_title("App with large Size", size = 20)

"""##Preparando datos para evaluación

"""

pruned_features = ['App', 'Genres', 'Current Ver', 'Android Ver']

target = 'Rating'
X = DF_GP.copy().drop(pruned_features+[target], axis=1)
y = DF_GP.copy()[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)

le_dict = defaultdict()
features_to_encode = X_train.select_dtypes(include=['category', 'object']).columns

for col in features_to_encode:
    le = LabelEncoder()

    X_train[col] = le.fit_transform(X_train[col]) # Ajustando y transformando datos de entrenamiento
    X_train[col] = X_train[col].astype('category') # Convertimos de tipo numérico a tipo categórico

    X_test[col] = le.transform(X_test[col]) # Transformamos datos de prueba
    X_test[col] = X_test[col].astype('category') # Convertimos de tipo numérico a tipo categórico

    le_dict[col] = le # Guardamos el label encoder por caracterisitcas individuales

from sklearn.preprocessing import StandardScaler

# Listado de características numéricas a escalar
numeric_features = X_train.select_dtypes(exclude=['category', 'object']).columns

scaler = StandardScaler()

# Ajustando y transformando datos de entrenamiento
X_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])


# Transformamos datos de prueba
X_test[numeric_features] = scaler.transform(X_test[numeric_features])

"""##Evaluación Modelo de Regresión

"""



models = ['Linear', 'KNN', 'Random Forest']
datasets = ['train', 'test']
metrics = ['RMSE', 'MAE', 'R2']

multi_index = pd.MultiIndex.from_product([models, datasets, metrics],
                                         names=['model', 'dataset', 'metric'])

df_metrics_reg = pd.DataFrame(index=multi_index,
                          columns=['value'])

df_metrics_reg

lr = LinearRegression()
lr.fit(X_train, y_train)

df_metrics_reg.loc['Linear', 'train', 'R2'] = lr.score(X_train, y_train)
df_metrics_reg.loc['Linear', 'test', 'R2'] = lr.score(X_test, y_test)

y_train_pred = lr.predict(X_train)
y_test_pred = lr.predict(X_test)

df_metrics_reg.loc['Linear', 'train', 'MAE'] = mean_absolute_error(y_train, y_train_pred)
df_metrics_reg.loc['Linear', 'test', 'MAE'] = mean_absolute_error(y_test, y_test_pred)

df_metrics_reg.loc['Linear', 'train', 'RMSE'] = mean_squared_error(y_train, y_train_pred, squared=False)
df_metrics_reg.loc['Linear', 'test', 'RMSE'] = mean_squared_error(y_test, y_test_pred, squared=False)

knn = KNeighborsRegressor()
knn.fit(X_train, y_train)

df_metrics_reg.loc['KNN', 'train', 'R2'] = knn.score(X_train, y_train)
df_metrics_reg.loc['KNN', 'test', 'R2'] = knn.score(X_test, y_test)

y_train_pred = knn.predict(X_train)
y_test_pred = knn.predict(X_test)

df_metrics_reg.loc['KNN', 'train', 'MAE'] = mean_absolute_error(y_train, y_train_pred)
df_metrics_reg.loc['KNN', 'test', 'MAE'] = mean_absolute_error(y_test, y_test_pred)

df_metrics_reg.loc['KNN', 'train', 'RMSE'] = mean_squared_error(y_train, y_train_pred, squared=False)
df_metrics_reg.loc['KNN', 'test', 'RMSE'] = mean_squared_error(y_test, y_test_pred, squared=False)

rf = RandomForestRegressor(max_depth=2, random_state=0)
rf.fit(X_train, y_train)

df_metrics_reg.loc['Random Forest', 'train', 'R2'] = rf.score(X_train, y_train)
df_metrics_reg.loc['Random Forest', 'test', 'R2'] = rf.score(X_test, y_test)

y_train_pred = rf.predict(X_train)
y_test_pred = rf.predict(X_test)

df_metrics_reg.loc['Random Forest', 'train', 'MAE'] = mean_absolute_error(y_train, y_train_pred)
df_metrics_reg.loc['Random Forest', 'test', 'MAE'] = mean_absolute_error(y_test, y_test_pred)

df_metrics_reg.loc['Random Forest', 'train', 'RMSE'] = mean_squared_error(y_train, y_train_pred, squared=False)
df_metrics_reg.loc['Random Forest', 'test', 'RMSE'] = mean_squared_error(y_test, y_test_pred, squared=False)

# Rounding the values

df_metrics_reg['value'] = df_metrics_reg['value'].apply(lambda v: round(v, ndigits=3))
df_metrics_reg

data = df_metrics_reg.reset_index()

g = sns.catplot(col='dataset', data=data, kind='bar', x='model', y='value', hue='metric')

# Adding annotations to bars
# iterate through axes
for ax in g.axes.ravel():
    # add annotations
    for c in ax.containers:
        ax.bar_label(c, label_type='edge')

    ax.margins(y=0.2)

plt.show()

"""##Evaluación Modelo Categórico"""

y_train_int = y_train.astype(int)
y_test_int = y_test.astype(int)

models = ['Logistic Regression', 'KNN', 'Random Forest']
datasets = ['train', 'test']

multi_index = pd.MultiIndex.from_product([models, datasets],
                                         names=['model', 'dataset'])

df_metrics_clf = pd.DataFrame(index=multi_index,
                          columns=['accuracy %'])

df_metrics_clf

lr_clf = LogisticRegression()
lr_clf.fit(X_train, y_train_int)

df_metrics_clf.loc['Logistic Regression', 'train'] = lr_clf.score(X_train, y_train_int)
df_metrics_clf.loc['Logistic Regression', 'test'] = lr_clf.score(X_test, y_test_int)

knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_train_int)

df_metrics_clf.loc['KNN', 'train'] = knn_clf.score(X_train, y_train_int)
df_metrics_clf.loc['KNN', 'test'] = knn_clf.score(X_test, y_test_int)

rf_clf = RandomForestClassifier()
rf_clf.fit(X_train, y_train_int)

df_metrics_clf.loc['Random Forest', 'train'] = rf_clf.score(X_train, y_train_int)
df_metrics_clf.loc['Random Forest', 'test'] = rf_clf.score(X_test, y_test_int)

# Rounding and coverting the accuracies to percentages
df_metrics_clf['accuracy %'] = df_metrics_clf['accuracy %'].apply(lambda v: round(v*100, ndigits=2))
df_metrics_clf

data = df_metrics_clf.reset_index()

g = sns.catplot(col='dataset', data=data, kind='bar', x='model', y='accuracy %')

# Adding annotations to bars
# iterate through axes
for ax in g.axes.ravel():
    # add annotations
    for c in ax.containers:
        ax.bar_label(c, label_type='edge')

    ax.margins(y=0.2)

plt.show()







"""##Modelo predictivo."""

#Primero probaremos el modelo más sencillo, una regresión logística. Así que vectorizaremos para este modelo y de nuevo lo haremos más adelante para el otro modelo.
#Partimos nuestro data set en test/train.

from sklearn.model_selection import train_test_split
df_train, df_test, y_train, y_test = train_test_split(DF_sentimiento['reviewlimpio'], DF_sentimiento['Sentiment'], test_size=0.2, random_state=42)

# Bola de palabras con pesado TFIDF

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()
X_tfidf_train = vectorizer.fit_transform(df_train)
X_tfidf_test = vectorizer.transform(df_test)

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(max_iter=1000)

lr.fit(X_tfidf_train,y_train)

predictions_tfidf = lr.predict(X_tfidf_test)

from sklearn.metrics import classification_report
print(classification_report(predictions_tfidf,y_test))

predicción_ejemplo ="A kid excessive ads The types ads allowed app"

# Pasamos nuestro texto en nuestro modelo y recibimos predicción.

vector_predicción_ejemplo=vectorizer.transform([predicción_ejemplo])

sentimiento=lr.predict(vector_predicción_ejemplo)

print('La opinion tiene un sentimiento: ', 'negativo' if sentimiento ==0  else 'positivo')

predicción_ejemplo2 ="This help eating healthy exercise regular basis"

# Pasamos nuestro texto en nuestro modelo y recibimos predicción.

vector_predicción_ejemplo=vectorizer.transform([predicción_ejemplo])

sentimiento=lr.predict(vector_predicción_ejemplo)

print('La opinion tiene un sentimiento: ', 'negativo' if sentimiento ==0  else 'positivo')





